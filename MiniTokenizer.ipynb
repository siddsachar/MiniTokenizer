{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df55c079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf0036f",
   "metadata": {},
   "source": [
    "## üìö Corpus Assembly: Merging Text Files for Tokenization\n",
    "\n",
    "This step combines multiple source documents into a single text file, preparing a unified corpus for downstream tokenization and analysis.\n",
    "\n",
    "### **Inputs**\n",
    "- **Directory:** `texts/` containing `.txt` files (UTF-8 encoded).\n",
    "- **Pattern:** All files matching `*.txt` are included; subfolders are ignored.\n",
    "\n",
    "### **Process Overview**\n",
    "1. **Enumerate Files:** Use `Path('./texts').glob('*.txt')` to list all text files in the directory.\n",
    "2. **Read & Concatenate:** For each file, read its contents as UTF-8 and append to a single output file.\n",
    "3. **Separation:** Add a end of sequence token (`<EOS>`) after each file to ensure clear separation between documents.\n",
    "4. **Output:** Write the combined result to `all_text.txt` in the project root.\n",
    "\n",
    "### **Why This Step?**\n",
    "- **Consistency:** Ensures all data is in one place for easier processing and reproducibility.\n",
    "- **Efficiency:** Downstream scripts (tokenizers, analyzers) can operate on a single file, simplifying I/O.\n",
    "- **Flexibility:** Easy to add or remove source files by updating the `texts/` folder.\n",
    "\n",
    "### **Validation & Tips**\n",
    "- Check the size of `all_text.txt` to confirm all data was written.\n",
    "- Open the file and inspect the start/end of each document for encoding or separator issues.\n",
    "- For reproducible order, use `sorted(Path('./texts').glob('*.txt'))`.\n",
    "- To add metadata, consider writing the filename or a header before each document.\n",
    "\n",
    "### **Next Steps**\n",
    "- Use `all_text.txt` as input for tokenizer training, vocabulary building, or text analysis.\n",
    "- Optionally, preprocess the text (e.g., normalization, lowercasing) before tokenization if required by your workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa3343a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = Path('./texts').glob('*.txt')\n",
    "with open('all_text.txt', 'w', encoding='utf-8') as outfile:\n",
    "    for file in files:\n",
    "        outfile.write(Path(file).read_text(encoding='utf-8') + '<EOS>')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf8f07a",
   "metadata": {},
   "source": [
    "## üìè Corpus Character Count: Quick Data Integrity Check\n",
    "\n",
    "After merging your text files into `all_text.txt`, it's important to verify that the corpus was assembled correctly and contains the expected amount of data. This cell performs a simple but effective validation by reading the entire file and reporting the total number of characters.\n",
    "\n",
    "### **Purpose**\n",
    "- **Sanity Check:** Confirms that `all_text.txt` is not empty and that the concatenation process worked as intended.\n",
    "- **Data Integrity:** Helps detect issues such as missing files, encoding errors, or incomplete writes.\n",
    "- **Baseline Metric:** Provides a reference point for future preprocessing or modifications.\n",
    "\n",
    "### **What This Cell Does**\n",
    "1. Opens `all_text.txt` in read mode with UTF-8 encoding.\n",
    "2. Reads the entire file content into a string variable (`raw_text`).\n",
    "3. Prints the total number of characters in the corpus.\n",
    "\n",
    "### **How to Use the Output**\n",
    "- **Expected Value:** The character count should be large and nonzero. If it's unexpectedly small, check your `texts/` directory for missing or empty files.\n",
    "- **Troubleshooting:** If you encounter encoding errors, ensure all input files are UTF-8 encoded.\n",
    "- **Scaling:** For very large corpora, consider reading the file in chunks or using file size in bytes (`os.stat('all_text.txt').st_size`) instead.\n",
    "\n",
    "### **Next Steps**\n",
    "- Use `raw_text` as input for tokenization, vocabulary extraction, or further text analysis.\n",
    "- Optionally, perform additional validation (e.g., line count, previewing the start/end of the file) to further ensure data quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "421a5db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of characters in the raw text: 330569\n"
     ]
    }
   ],
   "source": [
    "with open('all_text.txt', 'r', encoding='utf-8') as input_text:\n",
    "    raw_text = input_text.read()\n",
    "print(f\"Total number of characters in the raw text: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd711d6a",
   "metadata": {},
   "source": [
    "## üßÆ Tokenization and Vocabulary Construction\n",
    "\n",
    "This cell performs the core tokenization step and builds a vocabulary from your assembled corpus. It splits the raw text into tokens, counts them, and constructs a mapping from each unique token to a unique integer index.\n",
    "\n",
    "### **What This Cell Does**\n",
    "1. **Tokenization:**  \n",
    "   - Uses a regular expression with `re.split` to break the text into tokens.\n",
    "   - The pattern splits on common punctuation marks (`[,.!?():;_'\"\"]`), double dashes (`--`), and whitespace (`\\s`).\n",
    "   - This approach preserves punctuation as separate tokens and ensures that words, punctuation, and spaces are all represented.\n",
    "\n",
    "2. **Token Count:**  \n",
    "   - Prints the total number of tokens generated from the corpus.\n",
    "   - Useful for understanding the granularity and size of your tokenized data.\n",
    "\n",
    "3. **Unique Tokens:**  \n",
    "   - Converts the token list to a set to extract all unique tokens.\n",
    "   - Sorts them for reproducibility and prints the total count.\n",
    "   - This gives you the vocabulary size, a key metric for language modeling and analysis.\n",
    "\n",
    "4. **Vocabulary Mapping:**  \n",
    "   - Creates a dictionary (`vocab`) mapping each unique token to a unique integer index.\n",
    "   - This mapping is essential for converting text into numerical form for machine learning models.\n",
    "   - Adds a special (`<UNK>`) token to the end of the vocabulary to handle unknown tokens.\n",
    "\n",
    "### **Tips & Customization**\n",
    "- **Regex Tuning:** Adjust the regular expression to better fit your language or domain (e.g., handle contractions, special symbols, or multi-word expressions).\n",
    "- **Whitespace Handling:** The current pattern includes whitespace as tokens. If you want to ignore or merge whitespace, modify the regex accordingly.\n",
    "- **Vocabulary Filtering:** For large corpora, consider filtering out rare tokens or applying additional normalization (e.g., lowercasing, stemming).\n",
    "\n",
    "### **Next Steps**\n",
    "- Use the `tokenized_text` list for further processing, such as sequence modeling or n-gram analysis.\n",
    "- The `vocab` dictionary can be saved and reused for encoding new text or training models.\n",
    "- Analyze token frequency distributions or visualize the most common tokens for insights into your corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f32bfb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens: 143975\n",
      "Total number of unique tokens: 7357\n"
     ]
    }
   ],
   "source": [
    "tokenized_text = re.split(r'([,.!?():;_\\'\"]|--|\\s)', raw_text)\n",
    "print(f\"Total number of tokens: {len(tokenized_text)}\")\n",
    "\n",
    "all_tokens = sorted(set(tokenized_text))\n",
    "print(f\"Total number of unique tokens: {len(all_tokens)}\")\n",
    "\n",
    "vocab = {token : index for index, token in enumerate(all_tokens)}\n",
    "vocab.update({'<UNK>' : len(vocab)})\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79059742",
   "metadata": {},
   "source": [
    "## üß© MiniTokenizer Class: Encoding and Decoding Text\n",
    "\n",
    "This cell defines the `MiniTokenizer` class, which provides simple methods to convert text into sequences of token IDs (encoding) and to reconstruct text from token IDs (decoding) using the vocabulary built in previous steps.\n",
    "\n",
    "### **Class Overview**\n",
    "- **Initialization (`__init__`):**\n",
    "  - Takes a `vocab` dictionary mapping tokens to unique integer indices.\n",
    "  - Builds an `inverse_vocab` dictionary for reverse lookup (index to token), enabling decoding.\n",
    "\n",
    "- **Encoding (`encode` method):**\n",
    "  - Splits input text into tokens using the same regular expression as before (`re.split(r'([,.!?():;_\\'\"]|--|\\s)', text)`).\n",
    "  - Converts each token into its corresponding integer ID using the vocabulary.\n",
    "  - Adds the special token (`<UNK>`) if an unkown token is encountered.\n",
    "  - Returns a list of token IDs representing the input text.\n",
    "\n",
    "- **Decoding (`decode` method):**\n",
    "  - Converts a list of token IDs back into tokens using `inverse_vocab`.\n",
    "  - Joins the tokens into a string, separated by spaces.\n",
    "  - Returns the reconstructed text.\n",
    "\n",
    "### **Usage Example**\n",
    "```python\n",
    "tokenizer = MiniTokenizer(vocab)\n",
    "ids = tokenizer.encode(\"Hello, world!\")\n",
    "print(ids)  # Encoded token IDs\n",
    "print(tokenizer.decode(ids))  # Decoded text (may include extra spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f60e00fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.inverse_vocab = {index : token for token, index in vocab.items()}\n",
    "    def encode(self, text):\n",
    "        tokens = re.split(r'([,.!?():;_\\'\"]|--|\\s)', text)\n",
    "        token_ids = [self.vocab[token] if token in self.vocab else self.vocab['<UNK>'] for token in tokens]\n",
    "        return token_ids\n",
    "    def decode(self, token_ids):\n",
    "        text = ''.join([self.inverse_vocab[token_id] for token_id in token_ids])\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e9e80579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7357, 10, 0, 3, 7182, 4, 0, 3, 1335, 3, 4267, 3, 1473, 3, 6587, 3, 4955, 3, 3929, 3, 7092, 3, 6602, 3, 7357, 3, 7181, 12, 0]\n",
      "<UNK>, world! This is a test of how well the <UNK> works.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MiniTokenizer(vocab)\n",
    "ids = tokenizer.encode(\"Hello, world! This is a test of how well the tokenizer works.\")\n",
    "print(ids)  # Encoded token IDs\n",
    "print(tokenizer.decode(ids))  # Decoded text (may include extra spaces)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
